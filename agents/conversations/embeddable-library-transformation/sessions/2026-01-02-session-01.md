# Embeddable Library Transformation - Session 1

**Date**: 2026-01-02
**Focus**: Transform Durable from standalone application to embeddable library (Oban-style)
**Status**: completed

## Summary

Successfully transformed Durable from a standalone Elixir application with its own Ecto repo into an embeddable library that integrates with a host application's existing Ecto repo. This follows the pattern established by libraries like Oban, allowing users to add Durable to their supervision tree with minimal configuration.

## Key Decisions

1. **Dynamic Repo Pattern**: Instead of hardcoding `Durable.Repo`, all database operations now use `Durable.Config.repo()` to fetch the user's configured repo at runtime. Rationale: Allows embedding in any Elixir application with an existing Ecto repo.

2. **Schema Prefix**: Added `@schema_prefix "durable"` to all Ecto schemas to isolate Durable tables in their own PostgreSQL schema. Rationale: Prevents table name collisions and provides clean namespace separation.

3. **Persistent Term Storage**: Configuration is stored in `:persistent_term` for fast read access. Rationale: Configuration is read frequently during workflow execution; persistent_term provides near-zero-cost reads.

4. **NimbleOptions Validation**: All configuration options are validated using NimbleOptions at startup. Rationale: Provides clear error messages and documentation for configuration options.

5. **Programmatic Migrations**: Created `Durable.Migration.up()/down()` functions instead of requiring users to copy migration files. Rationale: Simplifies installation and ensures migrations stay in sync with library version.

6. **Supervisor as Entry Point**: `Durable.Supervisor` (aliased as `Durable` via `use Durable.Supervisor`) serves as the main entry point. Rationale: Follows OTP conventions and Oban's established pattern.

## Files Created

| File | Purpose |
|------|---------|
| `lib/durable/config.ex` | NimbleOptions validation, persistent_term storage, runtime config access |
| `lib/durable/migration.ex` | Programmatic `up()/down()` migration functions |
| `lib/durable/supervisor.ex` | Main supervision tree entry point |
| `test/support/test_repo.ex` | Test-only Ecto repo for test suite |

## Files Modified

| File | Changes |
|------|---------|
| `lib/durable/storage/schemas/*.ex` | Added `@schema_prefix "durable"` |
| `lib/durable/queue/adapters/postgres.ex` | Changed to use `Config.repo()`, added rescue clause |
| `lib/durable/executor.ex` | Changed to use `Config.repo()` |
| `lib/durable/query.ex` | Changed to use `Config.repo()` |
| `lib/durable/wait.ex` | Changed to use `Config.repo()` |
| `README.md` | Updated installation instructions |
| `CLAUDE.md` | Updated architecture documentation |
| `guides/ai_workflows.md` | Added setup section |

## Files Deleted

| File | Reason |
|------|--------|
| `lib/durable/repo.ex` | No longer needed; users provide their own repo |

## Installation Pattern Achieved

```elixir
# 1. Add migration to host application
defmodule MyApp.Repo.Migrations.AddDurable do
  use Ecto.Migration

  def up, do: Durable.Migration.up()
  def down, do: Durable.Migration.down()
end

# 2. Add to supervision tree in application.ex
children = [
  MyApp.Repo,
  {Durable, repo: MyApp.Repo, queues: %{default: [concurrency: 10]}}
]

Supervisor.start_link(children, strategy: :one_for_one)

# 3. Use workflows normally
Durable.start(MyWorkflow, %{input: "data"})
```

## Configuration Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `:repo` | module | *required* | User's Ecto repo module |
| `:name` | atom | `Durable` | Instance name for multi-instance support |
| `:prefix` | string | `"durable"` | PostgreSQL schema name |
| `:queues` | map | `%{}` | Queue configurations |
| `:queue_enabled` | boolean | `true` | Enable/disable queue processing |
| `:stale_lock_timeout` | integer | `300` | Seconds before lock considered stale |
| `:heartbeat_interval` | integer | `30_000` | Worker heartbeat interval in ms |

## CI Fixes Applied

1. **Postgres Adapter Rescue Clause**: Added rescue clause to `recover_stale_locks/2` to handle potential database errors gracefully.

2. **Credo Warning Fix**: Changed reraise pattern in `config.ex` from try/rescue to case pattern matching to satisfy Credo's style requirements.

3. **Moduledoc for Test Repo**: Added `@moduledoc false` to `test/support/test_repo.ex` to suppress documentation warnings.

4. **Alias for Log Handler**: Added explicit alias for `Durable.LogCapture.Handler` in `supervisor.ex` to fix undefined module warning.

## Test Results

All 80 tests pass after transformation:
- Queue adapter tests work with dynamic repo
- Workflow execution tests maintain full functionality
- No regressions in existing behavior

## Future Work: Pipe-Based API

Discussed future milestone from IMPLEMENTATION_PLAN.md for a functional, pipe-based workflow definition API:

```elixir
workflow =
  Durable.Workflow.new("process_order")
  |> Durable.Workflow.step(:validate, &validate_order/1)
  |> Durable.Workflow.step(:charge, &charge_payment/1, retry: [max_attempts: 3])
  |> Durable.Workflow.branch(:doc_type, %{
    invoice: [&extract_invoice/1, &validate_invoice/1],
    contract: [&extract_contract/1],
    _default: [&flag_review/1]
  })
  |> Durable.Workflow.parallel([&send_email/1, &notify_slack/1])
  |> Durable.Workflow.step(:finalize, &complete_order/1)

Durable.register(workflow)
Durable.start(workflow, %{order_id: 123})
```

This would provide an alternative to the macro-based DSL for users who prefer explicit function composition.

## Action Items

- [x] Create config.ex with NimbleOptions validation
- [x] Create migration.ex with programmatic migrations
- [x] Create supervisor.ex as main entry point
- [x] Update all schemas with schema_prefix
- [x] Update all modules to use dynamic repo
- [x] Delete hardcoded repo.ex
- [x] Create test_repo.ex for test suite
- [x] Fix all CI issues (Credo, dialyzer)
- [x] Update documentation (README, CLAUDE.md, guides)
- [x] Verify all 80 tests pass

## Open Questions

None - implementation is complete.

## Next Steps

1. Consider implementing pipe-based API as future enhancement
2. Add more comprehensive documentation/guides for embedding
3. Consider adding telemetry events for observability
4. Explore multi-instance support testing

---
*Archived by conversation-archiver agent*
